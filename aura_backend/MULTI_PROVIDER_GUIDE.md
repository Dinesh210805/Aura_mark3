# Generated by Copilot

# AURA Multi-Provider AI System

## Overview

AURA now supports multiple AI providers with dynamic switching capabilities, allowing you to choose between **Groq** and **Gemini** for different AI services. The system provides automatic failover, model selection, and per-request provider customization.

## Supported Providers & Services

### Groq Provider
- **STT (Speech-to-Text)**: ‚úÖ Whisper models
- **LLM (Large Language Model)**: ‚úÖ Llama, Mixtral models  
- **VLM (Vision-Language Model)**: ‚úÖ Llama Vision models
- **TTS (Text-to-Speech)**: ‚úÖ PlayAI voices

### Gemini Provider  
- **STT**: ‚ùå Not supported
- **LLM**: ‚úÖ Gemini 1.5/2.0 models
- **VLM**: ‚úÖ Gemini Vision models
- **TTS**: ‚ùå Not supported

## Available Models

### STT Models (Groq Only)
- `whisper-large-v3-turbo` - Fast and accurate (default)
- `whisper-large-v3` - High accuracy

### LLM Models

**Groq:**
- `llama-3.3-70b-versatile` - Versatile large model (default)
- `llama-3.1-8b-instant` - Fast efficient model
- `mixtral-8x7b-32768` - High-performance mixture of experts

**Gemini:**
- `gemini-1.5-pro-latest` - Most capable with 2M context
- `gemini-1.5-flash` - Fast and efficient (default)
- `gemini-1.5-flash-8b` - Smaller, faster model
- `gemini-2.0-flash-exp` - Experimental next-gen
- `gemini-exp-1206` - Experimental advanced model

### VLM Models

**Groq:**
- `llama-4-maverick-17b-128e-instruct` - UI analysis (default)
- `llama-vision-large` - Large vision model

**Gemini:**
- `gemini-1.5-flash` - Fast vision analysis (default)
- `gemini-1.5-pro-latest` - Advanced vision capabilities

### TTS Models (Groq Only)
- `playai-tts` - High-quality speech synthesis

## Configuration

### Environment Variables

```bash
# Default Providers
DEFAULT_STT_PROVIDER=groq
DEFAULT_LLM_PROVIDER=groq          # or gemini
DEFAULT_VLM_PROVIDER=groq          # or gemini  
DEFAULT_TTS_PROVIDER=groq

# Default Models
DEFAULT_STT_MODEL=whisper-large-v3-turbo
DEFAULT_LLM_MODEL=llama-3.3-70b-versatile
DEFAULT_VLM_MODEL=llama-4-maverick-17b-128e-instruct
DEFAULT_TTS_MODEL=playai-tts
DEFAULT_TTS_VOICE=Arista-PlayAI

# Enable automatic failover
ENABLE_PROVIDER_FALLBACK=true
```

## API Usage

### 1. Per-Request Provider Selection

You can specify providers and models for each request:

```bash
# Process with specific providers
curl -X POST http://localhost:8000/process \
  -F "audio=@sample.wav" \
  -F "screenshot=@screen.png" \
  -F "llm_provider=gemini" \
  -F "llm_model=gemini-1.5-flash" \
  -F "vlm_provider=groq" \
  -F "vlm_model=llama-4-maverick-17b-128e-instruct"

# Chat with specific LLM
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello", "session_id": "test"}' \
  --data-urlencode "llm_provider=gemini" \
  --data-urlencode "llm_model=gemini-2.0-flash-exp"
```

### 2. Provider Management API

#### Check Provider Health
```bash
curl http://localhost:8000/providers/health
```

#### Get Provider Information
```bash
curl http://localhost:8000/providers/info
```

#### List Available Models
```bash
# All models
curl http://localhost:8000/providers/models

# Models for specific service
curl "http://localhost:8000/providers/models?service_type=llm"

# Models for specific provider
curl "http://localhost:8000/providers/models?provider=gemini"
```

#### Switch Default Provider
```bash
curl -X POST http://localhost:8000/providers/switch \
  -H "Content-Type: application/json" \
  -d '{
    "service_type": "llm",
    "provider": "gemini", 
    "model": "gemini-1.5-flash"
  }'
```

#### Test Models
```bash
curl -X POST http://localhost:8000/providers/test \
  -H "Content-Type: application/json" \
  -d '{
    "service_type": "llm",
    "provider": "gemini",
    "model": "gemini-1.5-flash", 
    "test_input": "What is AI?"
  }'
```

#### Get Available Voices
```bash
curl http://localhost:8000/providers/voices
```

### 3. Service Configuration

#### Get Service Config
```bash
curl http://localhost:8000/providers/config/llm
```

#### Set Default Model
```bash
curl -X POST "http://localhost:8000/providers/config/llm/model?model=gemini-2.0-flash-exp"
```

## Usage Examples

### Example 1: Using Gemini for Intent Analysis

```python
import httpx

async def test_gemini_intent():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/chat",
            json={
                "text": "Open WhatsApp and send a message to John",
                "session_id": "test-gemini"
            },
            params={
                "llm_provider": "gemini",
                "llm_model": "gemini-1.5-pro-latest"
            }
        )
        return response.json()
```

### Example 2: Mixed Provider Setup

```python
# Use Gemini for intent analysis, Groq for vision
async def process_with_mixed_providers():
    files = {
        "audio": ("audio.wav", audio_data, "audio/wav"),
        "screenshot": ("screen.png", image_data, "image/png"),
        "llm_provider": (None, "gemini"),
        "llm_model": (None, "gemini-1.5-flash"),
        "vlm_provider": (None, "groq"),
        "vlm_model": (None, "llama-4-maverick-17b-128e-instruct")
    }
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/process",
            files=files
        )
        return response.json()
```

### Example 3: Automatic Failover

The system automatically falls back to alternative providers if the primary fails:

```python
# If Groq is down, automatically tries Gemini for LLM tasks
response = await client.post(
    "http://localhost:8000/chat",
    json={"text": "Hello", "session_id": "test"}
    # No provider specified - uses defaults with automatic failover
)
```

## Testing the System

Run the comprehensive test script:

```bash
cd d:\PROJECTS\Aura_mark3\aura_backend
python test_multi_provider.py
```

This will test:
- Provider health status
- Model availability
- Provider switching
- Cross-provider functionality
- Failover mechanisms

## Benefits

### 1. **Flexibility**
- Choose the best provider for each task
- Switch providers without code changes
- Per-request customization

### 2. **Reliability** 
- Automatic failover if providers fail
- Multiple providers reduce single points of failure
- Graceful error handling

### 3. **Performance**
- Use faster models when speed is needed
- Use more capable models for complex tasks
- Optimize costs by choosing appropriate providers

### 4. **Future-Proof**
- Easy to add new providers
- Model updates handled automatically
- Provider-agnostic architecture

## Best Practices

### 1. **Provider Selection**
- Use **Groq** for speed and comprehensive service coverage
- Use **Gemini** for advanced reasoning and long context needs
- Mix providers based on specific requirements

### 2. **Model Selection**
- **Fast tasks**: Use smaller, faster models (llama-3.1-8b-instant, gemini-1.5-flash-8b)
- **Complex tasks**: Use larger models (llama-3.3-70b-versatile, gemini-1.5-pro-latest)
- **Vision tasks**: Use specialized VLM models

### 3. **Fallback Strategy**
- Always enable fallback for production
- Set appropriate timeout values
- Monitor provider health regularly

### 4. **Cost Optimization**
- Monitor usage across providers
- Use environment variables for defaults
- Consider rate limits and pricing tiers

## Troubleshooting

### Common Issues

1. **Provider Unavailable**
   - Check API keys in .env file
   - Verify network connectivity
   - Check provider status pages

2. **Model Not Found**
   - Verify model name spelling
   - Check provider supports the model
   - Use `/providers/models` endpoint to list available models

3. **Fallback Not Working**
   - Ensure `ENABLE_PROVIDER_FALLBACK=true`
   - Check that fallback providers are configured
   - Verify fallback providers have valid API keys

### Debug Commands

```bash
# Check provider health
curl http://localhost:8000/providers/health

# Get detailed provider info
curl http://localhost:8000/providers/info

# Test specific model
curl -X POST http://localhost:8000/providers/test \
  -H "Content-Type: application/json" \
  -d '{"service_type": "llm", "provider": "gemini", "test_input": "test"}'
```

---

üéâ **Your AURA system now supports multi-provider AI with seamless switching between Groq and Gemini!**

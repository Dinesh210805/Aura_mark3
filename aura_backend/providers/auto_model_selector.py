# Generated by Copilot
"""
Auto Model Selector for AURA Multi-Provider System
Intelligently selects the best AI model based on task complexity, requirements, and context
Similar to Cursor and Perplexity's auto modes
"""

import re
import logging
from typing import Dict, Any, Optional, List, Tuple
from enum import Enum
from dataclasses import dataclass

logger = logging.getLogger(__name__)

class TaskComplexity(Enum):
    """Task complexity levels for model selection"""
    SIMPLE = "simple"           # Translation, classification, basic Q&A
    MEDIUM = "medium"           # General chat, analysis, summarization
    COMPLEX = "complex"         # Advanced reasoning, coding, research
    MULTIMODAL = "multimodal"   # Vision + text tasks

class PerformanceMode(Enum):
    """Performance optimization modes"""
    SPEED = "speed"             # Prioritize speed over everything
    BALANCED = "balanced"       # Balance speed, quality, cost
    QUALITY = "quality"         # Prioritize quality over speed/cost
    COST = "cost"              # Prioritize cost-effectiveness

@dataclass
class SelectionCriteria:
    """Criteria for model selection"""
    task_complexity: TaskComplexity
    performance_mode: PerformanceMode
    context_length_needed: int
    has_images: bool = False
    has_audio: bool = False
    has_video: bool = False
    cost_sensitive: bool = True
    requires_coding: bool = False
    requires_reasoning: bool = False
    language_detection: Optional[str] = None

class AutoModelSelector:
    """Intelligent model selector for AURA AI services"""
    
    def __init__(self):
        # Keywords that indicate different task types
        self.coding_keywords = {
            'code', 'programming', 'function', 'class', 'variable', 'debug', 
            'algorithm', 'python', 'javascript', 'java', 'c++', 'sql', 'html',
            'css', 'react', 'nodejs', 'api', 'database', 'framework', 'library',
            'github', 'git', 'repository', 'commit', 'merge', 'deployment'
        }
        
        self.reasoning_keywords = {
            'analyze', 'reasoning', 'logic', 'problem', 'solve', 'explain',
            'why', 'how', 'because', 'therefore', 'conclusion', 'inference',
            'deduction', 'analysis', 'research', 'study', 'investigate',
            'compare', 'contrast', 'evaluate', 'assess', 'critique'
        }
        
        self.simple_keywords = {
            'translate', 'translation', 'convert', 'transform', 'classify',
            'category', 'tag', 'label', 'identify', 'detect', 'extract',
            'find', 'search', 'list', 'count', 'sort', 'filter'
        }
        
        # Model recommendations by complexity and requirements
        self.model_recommendations = {
            # STT Models (Speech-to-Text)
            "stt": {
                TaskComplexity.SIMPLE: {
                    PerformanceMode.SPEED: ("groq", "whisper-large-v3-turbo"),
                    PerformanceMode.BALANCED: ("groq", "whisper-large-v3-turbo"),
                    PerformanceMode.QUALITY: ("groq", "whisper-large-v3"),
                    PerformanceMode.COST: ("groq", "whisper-large-v3-turbo")
                },
                TaskComplexity.MEDIUM: {
                    PerformanceMode.SPEED: ("groq", "whisper-large-v3-turbo"),
                    PerformanceMode.BALANCED: ("groq", "whisper-large-v3"),
                    PerformanceMode.QUALITY: ("groq", "whisper-large-v3"),
                    PerformanceMode.COST: ("groq", "whisper-large-v3-turbo")
                },
                TaskComplexity.COMPLEX: {
                    PerformanceMode.SPEED: ("groq", "whisper-large-v3"),
                    PerformanceMode.BALANCED: ("groq", "whisper-large-v3"),
                    PerformanceMode.QUALITY: ("groq", "whisper-large-v3"),
                    PerformanceMode.COST: ("groq", "whisper-large-v3")
                }
            },
            
            # LLM Models (Large Language Models)
            "llm": {
                TaskComplexity.SIMPLE: {
                    PerformanceMode.SPEED: ("groq", "llama-3.1-8b-instant"),
                    PerformanceMode.BALANCED: ("gemini", "gemini-2.5-flash-lite"),
                    PerformanceMode.QUALITY: ("gemini", "gemini-2.5-flash"),
                    PerformanceMode.COST: ("gemini", "gemini-2.5-flash-lite")
                },
                TaskComplexity.MEDIUM: {
                    PerformanceMode.SPEED: ("gemini", "gemini-2.5-flash"),
                    PerformanceMode.BALANCED: ("gemini", "gemini-2.5-flash"),
                    PerformanceMode.QUALITY: ("groq", "llama-3.3-70b-versatile"),
                    PerformanceMode.COST: ("gemini", "gemini-2.0-flash-lite")
                },
                TaskComplexity.COMPLEX: {
                    PerformanceMode.SPEED: ("gemini", "gemini-2.5-flash"),
                    PerformanceMode.BALANCED: ("gemini", "gemini-2.5-pro"),
                    PerformanceMode.QUALITY: ("gemini", "gemini-2.5-pro"),
                    PerformanceMode.COST: ("groq", "llama-3.3-70b-versatile")
                }
            },
            
            # VLM Models (Vision-Language Models)
            "vlm": {
                TaskComplexity.SIMPLE: {
                    PerformanceMode.SPEED: ("gemini", "gemini-2.5-flash"),
                    PerformanceMode.BALANCED: ("gemini", "gemini-2.5-flash"),
                    PerformanceMode.QUALITY: ("groq", "llama-4-maverick-17b-128e-instruct"),
                    PerformanceMode.COST: ("gemini", "gemini-2.0-flash-lite")
                },
                TaskComplexity.MEDIUM: {
                    PerformanceMode.SPEED: ("gemini", "gemini-2.5-flash"),
                    PerformanceMode.BALANCED: ("groq", "llama-4-maverick-17b-128e-instruct"),
                    PerformanceMode.QUALITY: ("gemini", "gemini-2.5-pro"),
                    PerformanceMode.COST: ("gemini", "gemini-2.0-flash")
                },
                TaskComplexity.COMPLEX: {
                    PerformanceMode.SPEED: ("gemini", "gemini-2.5-flash"),
                    PerformanceMode.BALANCED: ("gemini", "gemini-2.5-pro"),
                    PerformanceMode.QUALITY: ("gemini", "gemini-2.5-pro"),
                    PerformanceMode.COST: ("groq", "llama-4-maverick-17b-128e-instruct")
                },
                TaskComplexity.MULTIMODAL: {
                    PerformanceMode.SPEED: ("gemini", "gemini-2.5-flash"),
                    PerformanceMode.BALANCED: ("gemini", "gemini-2.5-pro"),
                    PerformanceMode.QUALITY: ("gemini", "gemini-2.5-pro"),
                    PerformanceMode.COST: ("gemini", "gemini-2.0-flash")
                }
            },
            
            # TTS Models (Text-to-Speech)
            "tts": {
                TaskComplexity.SIMPLE: {
                    PerformanceMode.SPEED: ("groq", "playai-tts"),
                    PerformanceMode.BALANCED: ("groq", "playai-tts"),
                    PerformanceMode.QUALITY: ("groq", "playai-tts"),
                    PerformanceMode.COST: ("groq", "playai-tts")
                },
                TaskComplexity.MEDIUM: {
                    PerformanceMode.SPEED: ("groq", "playai-tts"),
                    PerformanceMode.BALANCED: ("groq", "playai-tts"),
                    PerformanceMode.QUALITY: ("groq", "playai-tts"),
                    PerformanceMode.COST: ("groq", "playai-tts")
                },
                TaskComplexity.COMPLEX: {
                    PerformanceMode.SPEED: ("groq", "playai-tts"),
                    PerformanceMode.BALANCED: ("groq", "playai-tts"),
                    PerformanceMode.QUALITY: ("groq", "playai-tts"),
                    PerformanceMode.COST: ("groq", "playai-tts")
                }
            }
        }

    def analyze_task_complexity(self, text: str, context: Dict[str, Any] = None) -> SelectionCriteria:
        """
        Analyze input text and context to determine task complexity and requirements
        """
        if not text:
            text = ""
        
        text_lower = text.lower()
        word_count = len(text.split())
        context = context or {}
        
        # Initialize criteria
        criteria = SelectionCriteria(
            task_complexity=TaskComplexity.MEDIUM,
            performance_mode=PerformanceMode.BALANCED,
            context_length_needed=word_count * 4,  # Rough token estimate
            has_images=bool(context.get("_screenshot_bytes") or context.get("image")),
            has_audio=bool(context.get("audio_data") or context.get("_audio_bytes")),
            has_video=bool(context.get("video_data")),
            cost_sensitive=True
        )
        
        # Detect coding requirements
        coding_score = sum(1 for keyword in self.coding_keywords if keyword in text_lower)
        criteria.requires_coding = coding_score > 0
        
        # Detect reasoning requirements
        reasoning_score = sum(1 for keyword in self.reasoning_keywords if keyword in text_lower)
        criteria.requires_reasoning = reasoning_score > 1
        
        # Detect simple tasks
        simple_score = sum(1 for keyword in self.simple_keywords if keyword in text_lower)
        
        # Determine complexity level
        if criteria.has_images or criteria.has_audio or criteria.has_video:
            criteria.task_complexity = TaskComplexity.MULTIMODAL
        elif coding_score > 2 or reasoning_score > 2 or word_count > 1000:
            criteria.task_complexity = TaskComplexity.COMPLEX
        elif simple_score > 1 and coding_score == 0 and reasoning_score == 0:
            criteria.task_complexity = TaskComplexity.SIMPLE
        else:
            criteria.task_complexity = TaskComplexity.MEDIUM
        
        # Adjust context length requirements
        if criteria.task_complexity == TaskComplexity.COMPLEX:
            criteria.context_length_needed = max(criteria.context_length_needed, 4000)
        elif criteria.task_complexity == TaskComplexity.MULTIMODAL:
            criteria.context_length_needed = max(criteria.context_length_needed, 8000)
        
        logger.info(f"Task analysis: complexity={criteria.task_complexity.value}, "
                   f"coding={criteria.requires_coding}, reasoning={criteria.requires_reasoning}, "
                   f"multimodal={criteria.has_images or criteria.has_audio}")
        
        return criteria

    def select_model(
        self, 
        service_type: str, 
        text: str = "", 
        context: Dict[str, Any] = None,
        performance_mode: str = "balanced",
        cost_sensitive: bool = True
    ) -> Tuple[Optional[str], Optional[str]]:
        """
        Select the best provider and model for the given task
        
        Returns:
            Tuple of (provider_name, model_name) or (None, None) if no recommendation
        """
        try:
            # Analyze the task
            criteria = self.analyze_task_complexity(text, context)
            
            # Override performance mode if specified
            try:
                criteria.performance_mode = PerformanceMode(performance_mode.lower())
            except ValueError:
                criteria.performance_mode = PerformanceMode.BALANCED
            
            criteria.cost_sensitive = cost_sensitive
            
            # Get recommendations for this service type
            service_recommendations = self.model_recommendations.get(service_type)
            if not service_recommendations:
                logger.warning(f"No recommendations available for service type: {service_type}")
                return None, None
            
            # Get complexity-specific recommendations
            complexity_recommendations = service_recommendations.get(criteria.task_complexity)
            if not complexity_recommendations:
                # Fallback to medium complexity
                complexity_recommendations = service_recommendations.get(TaskComplexity.MEDIUM)
                if not complexity_recommendations:
                    logger.warning(f"No recommendations for {service_type} with complexity {criteria.task_complexity}")
                    return None, None
            
            # Select based on performance mode
            recommendation = complexity_recommendations.get(criteria.performance_mode)
            if not recommendation:
                # Fallback to balanced mode
                recommendation = complexity_recommendations.get(PerformanceMode.BALANCED)
                if not recommendation:
                    logger.warning(f"No recommendation for {service_type} with mode {criteria.performance_mode}")
                    return None, None
            
            provider, model = recommendation
            
            # Apply special rules based on criteria
            if criteria.requires_coding and service_type == "llm":
                # Prefer models known for coding
                if criteria.performance_mode in [PerformanceMode.QUALITY, PerformanceMode.BALANCED]:
                    provider, model = "gemini", "gemini-2.5-pro"
                else:
                    provider, model = "groq", "llama-3.3-70b-versatile"
            
            if criteria.context_length_needed > 100000 and service_type == "llm":
                # Use models with large context windows
                if provider == "gemini":
                    model = "gemini-2.5-pro"  # 2M+ tokens
                elif provider == "groq":
                    model = "llama-3.1-8b-instant"  # 131k tokens
            
            if criteria.cost_sensitive and criteria.task_complexity == TaskComplexity.SIMPLE:
                # Override with most cost-effective options
                if service_type == "llm":
                    provider, model = "gemini", "gemini-2.5-flash-lite"
                elif service_type == "vlm":
                    provider, model = "gemini", "gemini-2.0-flash-lite"
            
            logger.info(f"Auto-selected for {service_type}: {provider}/{model} "
                       f"(complexity: {criteria.task_complexity.value}, "
                       f"mode: {criteria.performance_mode.value})")
            
            return provider, model
            
        except Exception as e:
            logger.error(f"Error in auto model selection: {str(e)}")
            return None, None

    def get_fallback_models(self, service_type: str, primary_provider: str) -> List[Tuple[str, str]]:
        """
        Get fallback models for a service if the primary fails
        """
        fallbacks = []
        
        if service_type == "llm":
            if primary_provider == "groq":
                fallbacks = [
                    ("gemini", "gemini-2.5-flash"),
                    ("gemini", "gemini-2.0-flash")
                ]
            else:  # gemini
                fallbacks = [
                    ("groq", "llama-3.3-70b-versatile"),
                    ("groq", "llama-3.1-8b-instant")
                ]
        
        elif service_type == "vlm":
            if primary_provider == "groq":
                fallbacks = [
                    ("gemini", "gemini-2.5-flash"),
                    ("gemini", "gemini-2.0-flash")
                ]
            else:  # gemini
                fallbacks = [
                    ("groq", "llama-4-maverick-17b-128e-instruct"),
                    ("groq", "llama-vision-large")
                ]
        
        elif service_type == "stt":
            fallbacks = [("groq", "whisper-large-v3")]
        
        elif service_type == "tts":
            fallbacks = [("groq", "playai-tts")]
        
        return fallbacks

    def explain_selection(
        self, 
        service_type: str, 
        provider: str, 
        model: str, 
        criteria: SelectionCriteria
    ) -> str:
        """
        Provide human-readable explanation for model selection
        """
        explanations = []
        
        # Base selection reason
        if criteria.task_complexity == TaskComplexity.SIMPLE:
            explanations.append(f"Selected {provider}/{model} for simple task requiring speed and cost-efficiency")
        elif criteria.task_complexity == TaskComplexity.COMPLEX:
            explanations.append(f"Selected {provider}/{model} for complex task requiring advanced reasoning")
        elif criteria.task_complexity == TaskComplexity.MULTIMODAL:
            explanations.append(f"Selected {provider}/{model} for multimodal task with vision capabilities")
        else:
            explanations.append(f"Selected {provider}/{model} for balanced performance")
        
        # Additional factors
        if criteria.requires_coding:
            explanations.append("coding capabilities needed")
        
        if criteria.requires_reasoning:
            explanations.append("advanced reasoning required")
        
        if criteria.context_length_needed > 50000:
            explanations.append("large context window needed")
        
        if criteria.cost_sensitive:
            explanations.append("cost optimization prioritized")
        
        if criteria.performance_mode == PerformanceMode.SPEED:
            explanations.append("speed prioritized")
        elif criteria.performance_mode == PerformanceMode.QUALITY:
            explanations.append("quality prioritized")
        
        return " - ".join(explanations)

# Global instance
auto_selector = AutoModelSelector()

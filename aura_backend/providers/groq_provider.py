# Generated by Copilot
"""
Groq Provider Implementation
Implements all AI services using Groq API
"""

import httpx
import json
import base64
import asyncio
from typing import Optional, Dict, Any, List
import logging

from .base import (
    BaseSTTProvider, BaseLLMProvider, BaseVLMProvider, BaseTTSProvider,
    AIModel, ProviderConfig, ProviderError, ModelNotAvailableError,
    RateLimitError, ProviderUnavailableError
)

logger = logging.getLogger(__name__)

class GroqProvider(BaseSTTProvider, BaseLLMProvider, BaseVLMProvider, BaseTTSProvider):
    """Unified Groq provider for all AI services"""
    
    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self._models = self._initialize_models()
    
    def _initialize_models(self) -> List[AIModel]:
        """Initialize available Groq models"""
        return [
            # STT Models
            AIModel(
                name="whisper-large-v3-turbo",
                provider="groq", 
                capabilities=["stt"],
                description="Fast and accurate speech recognition"
            ),
            AIModel(
                name="whisper-large-v3",
                provider="groq",
                capabilities=["stt"], 
                description="High accuracy speech recognition"
            ),
            
            # LLM Models
            AIModel(
                name="llama-3.3-70b-versatile",
                provider="groq",
                capabilities=["llm"],
                max_tokens=8192,
                context_length=8192,
                description="Versatile large language model"
            ),
            AIModel(
                name="llama-3.1-8b-instant",
                provider="groq",
                capabilities=["llm"],
                max_tokens=8192,
                context_length=131072,
                description="Fast and efficient language model"
            ),
            AIModel(
                name="mixtral-8x7b-32768",
                provider="groq",
                capabilities=["llm"],
                max_tokens=32768,
                context_length=32768,
                description="High-performance mixture of experts model"
            ),
            
            # VLM Models
            AIModel(
                name="llama-4-maverick-17b-128e-instruct",
                provider="groq",
                capabilities=["vlm"],
                max_tokens=8192,
                description="Vision-language model for UI analysis"
            ),
            AIModel(
                name="llama-vision-large",
                provider="groq",
                capabilities=["vlm"],
                max_tokens=4096,
                description="Large vision-language model"
            ),
            
            # TTS Models
            AIModel(
                name="playai-tts",
                provider="groq",
                capabilities=["tts"],
                description="High-quality text-to-speech via PlayAI"
            )
        ]
    
    async def health_check(self) -> Dict[str, Any]:
        """Check Groq API health"""
        if not self.is_available():
            return {"status": "unavailable", "reason": "No API key"}
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                headers = {"Authorization": f"Bearer {self.api_key}"}
                response = await client.get(
                    f"{self.base_url}/models",
                    headers=headers
                )
                
                if response.status_code == 200:
                    return {"status": "healthy", "models_accessible": True}
                else:
                    return {"status": "unhealthy", "error": f"API returned {response.status_code}"}
                    
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    def get_available_models(self) -> List[AIModel]:
        """Get available Groq models"""
        return self._models
    
    def get_models_by_capability(self, capability: str) -> List[AIModel]:
        """Get models that support specific capability"""
        return [model for model in self._models if capability in model.capabilities]
    
    # STT Implementation
    async def transcribe(
        self, 
        audio_data: bytes, 
        model: Optional[str] = None,
        language: Optional[str] = "en",
        **kwargs
    ) -> Optional[str]:
        """Transcribe audio using Groq Whisper"""
        if not self.is_available():
            raise ProviderUnavailableError("Groq API key not available", "groq")
        
        model = model or "whisper-large-v3-turbo"
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                files = {
                    "file": ("audio.wav", audio_data, "audio/wav"),
                    "model": (None, model),
                    "response_format": (None, "text"),
                    "language": (None, language)
                }
                
                headers = {"Authorization": f"Bearer {self.api_key}"}
                
                logger.info(f"Groq STT: Transcribing with model {model}")
                response = await client.post(
                    f"{self.base_url}/audio/transcriptions",
                    files=files,
                    headers=headers
                )
                
                if response.status_code == 200:
                    transcript = response.text.strip()
                    logger.info(f"Groq STT: Success - '{transcript[:50]}...'")
                    return transcript
                elif response.status_code == 429:
                    raise RateLimitError("Rate limit exceeded", "groq")
                else:
                    logger.error(f"Groq STT Error: {response.status_code} - {response.text}")
                    return None
                    
        except asyncio.TimeoutError:
            logger.error("Groq STT: Request timeout")
            return None
        except Exception as e:
            logger.error(f"Groq STT Exception: {str(e)}")
            return None
    
    def get_supported_formats(self) -> List[str]:
        """Get supported audio formats for STT"""
        return ["wav", "mp3", "flac", "m4a", "ogg", "webm"]
    
    # LLM Implementation
    async def analyze_intent(
        self, 
        transcript: str, 
        ui_tree: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Analyze intent using Groq LLM"""
        if not self.is_available():
            raise ProviderUnavailableError("Groq API key not available", "groq")
        
        model = model or "llama-3.3-70b-versatile"
        
        system_prompt = """You are AURA, an intelligent Android accessibility assistant. 
        Analyze the user's voice command and determine:
        1. The specific intent/goal
        2. What UI elements need to be interacted with
        3. The sequence of actions required
        4. Whether screen analysis is needed

        Respond in JSON format:
        {
            "intent": "brief description of what user wants",
            "target_elements": ["element1", "element2"],
            "requires_screen_analysis": true/false,
            "action_type": "tap|swipe|type|navigate|open_app|system_command",
            "parameters": {"key": "value"},
            "confidence": 0.0-1.0
        }"""
        
        user_prompt = f"User said: '{transcript}'"
        if ui_tree:
            user_prompt += f"\n\nAvailable UI elements: {ui_tree[:2000]}..."
        
        try:
            response = await self.chat_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                model=model,
                temperature=0.1,
                max_tokens=1000,
                response_format={"type": "json_object"}
            )
            
            if response.get("success"):
                content = response["content"]
                try:
                    intent_data = json.loads(content)
                    logger.info(f"Groq LLM: Intent analyzed - {intent_data.get('intent', 'Unknown')}")
                    return intent_data
                except json.JSONDecodeError:
                    logger.warning("Groq LLM: Failed to parse JSON response")
                    return {
                        "intent": content,
                        "requires_screen_analysis": True,
                        "action_type": "tap",
                        "confidence": 0.5
                    }
            else:
                return {"error": response.get("error", "LLM request failed")}
                
        except Exception as e:
            logger.error(f"Groq LLM Exception: {str(e)}")
            return {"error": str(e)}
    
    async def generate_response(
        self, 
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Optional[str]:
        """Generate text response"""
        try:
            response = await self.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            if response.get("success"):
                return response["content"]
            else:
                logger.error(f"Groq response generation failed: {response.get('error')}")
                return None
                
        except Exception as e:
            logger.error(f"Groq response generation exception: {str(e)}")
            return None
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Chat completion with Groq"""
        if not self.is_available():
            raise ProviderUnavailableError("Groq API key not available", "groq")
        
        model = model or "llama-3.3-70b-versatile"
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                payload = {
                    "model": model,
                    "messages": messages,
                    "temperature": kwargs.get("temperature", 0.1),
                    "max_tokens": kwargs.get("max_tokens", 1000),
                    **{k: v for k, v in kwargs.items() if k not in ["temperature", "max_tokens"]}
                }
                
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                logger.info(f"Groq LLM: Chat completion with model {model}")
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    json=payload,
                    headers=headers
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result["choices"][0]["message"]["content"]
                    return {"success": True, "content": content, "model": model}
                elif response.status_code == 429:
                    raise RateLimitError("Rate limit exceeded", "groq")
                else:
                    error_msg = f"API error: {response.status_code}"
                    logger.error(f"Groq LLM Error: {error_msg} - {response.text}")
                    return {"success": False, "error": error_msg}
                    
        except asyncio.TimeoutError:
            return {"success": False, "error": "Request timeout"}
        except Exception as e:
            logger.error(f"Groq LLM Exception: {str(e)}")
            return {"success": False, "error": str(e)}
    
    # VLM Implementation
    async def locate_ui_element(
        self, 
        screenshot: bytes, 
        intent: str,
        model: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Locate UI elements using Groq VLM"""
        if not self.is_available():
            raise ProviderUnavailableError("Groq API key not available", "groq")
        
        model = model or "llama-4-maverick-17b-128e-instruct"
        
        try:
            image_b64 = base64.b64encode(screenshot).decode('utf-8')
        except Exception as e:
            return {"found": False, "error": f"Image encoding failed: {str(e)}"}
        
        system_prompt = """You are a UI element locator. Analyze the screenshot and find the UI element 
        that matches the user's intent. Look for buttons, text fields, icons, or other interactive elements.
        
        Return coordinates in JSON format:
        {
            "found": true/false,
            "coordinates": {"x": int, "y": int, "width": int, "height": int},
            "confidence": 0.0-1.0,
            "element_description": "what you found",
            "reasoning": "why you chose this element"
        }"""
        
        user_prompt = f"""Find the UI element for this action: "{intent}"

Look for:
- Buttons with relevant text
- Input fields if typing is needed
- Icons or images that match the intent
- Navigation elements
- Any clickable areas

Provide exact pixel coordinates for the center of the element."""
        
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": user_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_b64}"
                            }
                        }
                    ]
                }
            ]
            
            response = await self.chat_completion(
                messages=messages,
                model=model,
                temperature=0.1,
                max_tokens=500,
                response_format={"type": "json_object"}
            )
            
            if response.get("success"):
                content = response["content"]
                try:
                    vlm_result = json.loads(content)
                    if vlm_result.get("found"):
                        logger.info(f"Groq VLM: Found UI element - {vlm_result.get('element_description', 'Unknown')}")
                    else:
                        logger.warning("Groq VLM: No matching UI element found")
                    return vlm_result
                except json.JSONDecodeError:
                    return {"found": False, "error": "Could not parse VLM response"}
            else:
                return {"found": False, "error": response.get("error", "VLM request failed")}
                
        except Exception as e:
            logger.error(f"Groq VLM Exception: {str(e)}")
            return {"found": False, "error": str(e)}
    
    async def analyze_screen_context(
        self, 
        screenshot: bytes,
        model: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Analyze screen context using Groq VLM"""
        if not self.is_available():
            return {"error": "Groq API key not available"}
        
        model = model or "llama-4-maverick-17b-128e-instruct"
        
        try:
            image_b64 = base64.b64encode(screenshot).decode('utf-8')
            
            system_prompt = """Analyze this Android screenshot and describe:
            1. What app/screen is displayed
            2. Main UI elements visible
            3. Possible actions user can take
            
            Return JSON format:
            {
                "app_name": "detected app",
                "screen_type": "main/settings/dialog/etc",
                "ui_elements": ["button1", "input_field", "menu"],
                "suggestions": ["possible actions"]
            }"""
            
            messages = [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Analyze this screen"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_b64}"
                            }
                        }
                    ]
                }
            ]
            
            response = await self.chat_completion(
                messages=messages,
                model=model,
                temperature=0.2,
                max_tokens=300
            )
            
            if response.get("success"):
                content = response["content"]
                try:
                    return json.loads(content)
                except json.JSONDecodeError:
                    return {"screen_analysis": content}
            else:
                return {"error": response.get("error", "Screen analysis failed")}
                
        except Exception as e:
            logger.error(f"Groq screen analysis error: {str(e)}")
            return {"error": str(e)}
    
    async def describe_image(
        self, 
        image_data: bytes,
        prompt: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ) -> Optional[str]:
        """Describe image using Groq VLM"""
        try:
            image_b64 = base64.b64encode(image_data).decode('utf-8')
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt or "Describe this image in detail"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_b64}"
                            }
                        }
                    ]
                }
            ]
            
            response = await self.chat_completion(
                messages=messages,
                model=model or "llama-4-maverick-17b-128e-instruct",
                **kwargs
            )
            
            if response.get("success"):
                return response["content"]
            else:
                return None
                
        except Exception as e:
            logger.error(f"Groq image description error: {str(e)}")
            return None
    
    # TTS Implementation
    async def generate_speech(
        self, 
        text: str, 
        voice: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ) -> Optional[bytes]:
        """Generate speech using Groq PlayAI TTS"""
        if not self.is_available():
            raise ProviderUnavailableError("Groq API key not available", "groq")
        
        if not text or not text.strip():
            logger.warning("Groq TTS: Empty text provided")
            return None
        
        model = model or "playai-tts"
        voice = voice or "Arista-PlayAI"
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                payload = {
                    "model": model,
                    "input": text.strip(),
                    "voice": voice,
                    "response_format": "wav"
                }
                
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                logger.info(f"Groq TTS: Generating speech with voice {voice}")
                response = await client.post(
                    f"{self.base_url}/audio/speech",
                    json=payload,
                    headers=headers
                )
                
                if response.status_code == 200:
                    logger.info("Groq TTS: Speech generation successful")
                    return response.content
                elif response.status_code == 429:
                    raise RateLimitError("Rate limit exceeded", "groq")
                else:
                    logger.error(f"Groq TTS Error: {response.status_code} - {response.text}")
                    return None
                    
        except asyncio.TimeoutError:
            logger.error("Groq TTS: Request timeout")
            return None
        except Exception as e:
            logger.error(f"Groq TTS Exception: {str(e)}")
            return None
    
    def get_available_voices(self) -> List[Dict[str, str]]:
        """Get available PlayAI voices"""
        return [
            {"name": "Arista-PlayAI", "language": "en", "gender": "female", "description": "High-quality female voice"},
            {"name": "Atlas-PlayAI", "language": "en", "gender": "male", "description": "Professional male voice"},
            {"name": "Basil-PlayAI", "language": "en", "gender": "male", "description": "Warm male voice"},
            {"name": "Briggs-PlayAI", "language": "en", "gender": "male", "description": "Confident male voice"},
            {"name": "Calum-PlayAI", "language": "en", "gender": "male", "description": "Clear male voice"},
            {"name": "Celeste-PlayAI", "language": "en", "gender": "female", "description": "Elegant female voice"},
            {"name": "Cheyenne-PlayAI", "language": "en", "gender": "female", "description": "Friendly female voice"},
            {"name": "Chip-PlayAI", "language": "en", "gender": "male", "description": "Energetic male voice"},
            {"name": "Cillian-PlayAI", "language": "en", "gender": "male", "description": "Smooth male voice"},
            {"name": "Deedee-PlayAI", "language": "en", "gender": "female", "description": "Cheerful female voice"},
            {"name": "Fritz-PlayAI", "language": "en", "gender": "male", "description": "Technical male voice"},
            {"name": "Gail-PlayAI", "language": "en", "gender": "female", "description": "Mature female voice"},
            {"name": "Indigo-PlayAI", "language": "en", "gender": "neutral", "description": "Neutral voice"},
            {"name": "Mamaw-PlayAI", "language": "en", "gender": "female", "description": "Grandmother-like voice"},
            {"name": "Mason-PlayAI", "language": "en", "gender": "male", "description": "Strong male voice"},
            {"name": "Mikail-PlayAI", "language": "en", "gender": "male", "description": "International male voice"},
            {"name": "Mitch-PlayAI", "language": "en", "gender": "male", "description": "Casual male voice"},
            {"name": "Quinn-PlayAI", "language": "en", "gender": "neutral", "description": "Young neutral voice"},
            {"name": "Thunder-PlayAI", "language": "en", "gender": "male", "description": "Deep male voice"}
        ]
    
    def get_supported_formats(self) -> List[str]:
        """Get supported TTS output formats"""
        return ["wav", "mp3", "flac"]
